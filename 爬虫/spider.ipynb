{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 知识点总结\n",
    "- requests模块\n",
    "    - 爬取数据流程：\n",
    "    - get，post作用：\n",
    "    - get，post参数：\n",
    "        - url\n",
    "        - headers\n",
    "        - data/params\n",
    "        - proxies\n",
    "    - 处理ajax动态加载的数据：\n",
    "        - 抓包工具\n",
    "        - 获取数据包中对应的url\n",
    "        - response.json()\n",
    "    - 模拟登陆：\n",
    "        - \n",
    "    - 打码平台使用：\n",
    "        - 开发者用户：\n",
    "            - 创建一个软件（id,key）\n",
    "            - 下载实例代码（HttpPython）\n",
    "    - cookie的处理：\n",
    "        - session:requests.Session()\n",
    "    - 代理ip：\n",
    "    - 线程池：\n",
    "        from mulprocessing.dummy import POOl\n",
    "        map(func,list)\n",
    "    - 图片懒加载：\n",
    "        - 伪属性\n",
    "- 数据解析\n",
    "    - 正则表达式：\n",
    "    - bs4：\n",
    "        - 实例化解析对象（将页面源码数据加载到该对象中）\n",
    "        - 通过对象中的相关方法实现页面中标签的定位\n",
    "        - 取文本 取属性\n",
    "        \n",
    "        - find('element',attr='xxx'):返回的是单数\n",
    "        - find_all()\n",
    "        - soup.element\n",
    "        - select(选择器)：\n",
    "            - 层级选择器：\n",
    "                - 分隔符：>,单个层级  空格，多个层级\n",
    "        - 取文本：element.string     .text   .get_text()        \n",
    "                \n",
    "    - xpath：\n",
    "        - xpath('xpath表达式')\n",
    "        - /  or  //\n",
    "        - 属性定位： //div[@class='xxx']/a//span\n",
    "        - 索引定位：//div[1]\n",
    "        - 模糊定位：//div[contains('class_','xxx')]   //div[start_with('class_','xxx')]\n",
    "        - /text()  //text()\n",
    "        - //div/@class\n",
    "        \n",
    "        - xpath插件\n",
    "        \n",
    "    - 局部页面内容的解析：\n",
    "        1.定位到一批li标签\n",
    "        2.单独对li标签表示的页面数据进行数据解析  li.xpath('.//')\n",
    "        \n",
    "- selenium\n",
    "    - 作用：完成浏览器自动化的操作（通过编写代码制定一些列的行为动作，将该行为动作同步映射到浏览器中）\n",
    "    - 使用流程：\n",
    "        1.from selenium import webdriver\n",
    "        2.实例化某一款浏览器对象(浏览器的驱动程序路径作为参数)\n",
    "        3.制定行为动作\n",
    "            - bro.get(url=url)\n",
    "            - bro.excute_script(‘js’)\n",
    "            - page_source:当前页面源码数据（动态加载出来的数据）\n",
    "    - find系列函数：\n",
    "    - switch_to.frame(id)函数:\n",
    "        \n",
    "    - phantomJs：\n",
    "    - 谷歌无头浏览器：\n",
    "- scrapy\n",
    "    - 项目创建流程：\n",
    "        - scrapy startproject xxxPro\n",
    "        - cd xxxPro\n",
    "        - scrapy genspider xxx www.xxx.com\n",
    "        response.xpath('exp'):返回的列表（Selector）\n",
    "        extract/extract_first()：\n",
    "    - 持久化存储：\n",
    "        - 基于终端指令：scrapy crawl xxx -o xxx.csv\n",
    "            - 只可以将parse方法的返回值进行持久化存储\n",
    "        - 基于管道：\n",
    "            1.获得解析到的数据\n",
    "            2.将解析到的数据封装到item对象中\n",
    "            3.将item提交到管道\n",
    "            4.管道类中的process_item方法中的item参数进行item对象的接收\n",
    "            5.开启管道\n",
    "                ITEM_PIULELINE = {\n",
    "                    qiubaiPro.pipelines.qiubaiLine:301,\n",
    "                    qiubaiPro.pipelines.qiubaiLine110:302\n",
    "                }\n",
    "            \n",
    "            - open_spider(self,spider)\n",
    "            - close_spider(self,spider)\n",
    "            - process_item方法中的返回值：\n",
    "    - 处理分页数据爬取：\n",
    "        - 手动请求发送：yield scrapy.Request(url,callback)   yield scrapy.FormRequest(url,formdata,callback)\n",
    "    - post请求\n",
    "    - cookie处理：\n",
    "        - 不需要手动处理cookie\n",
    "    - 日志等级：\n",
    "        LOG_LEVEL = 'ERROR'\n",
    "        LOG_FILE = './log.txt'\n",
    "    - 请求传参：（传递的是什么呢？答案：item）\n",
    "        - 场景：在使用scrapy进行数据爬取时，如果解析的数据不在同一张页面中。\n",
    "        - scrapy.Requests（url,callback,meta={'item':item}）\n",
    "        - 如何接收item：item = response.meta['item']\n",
    "    - 五大核心组件原理：\n",
    "        spider 引擎 调度器 管道 下载器\n",
    "    - 下载中间件：\n",
    "        - process_request(self,request,spider):\n",
    "            request.headers['User-Agent'] = 'xxxx'\n",
    "            request.meta['proxy'] = 'http://ip:port'\n",
    "           \n",
    "           from scrapy.http import HtmlResponse\n",
    "        - process_response(self,request,response,spider):\n",
    "            return HtmlResponse(url,body,request,encoding)\n",
    "            \n",
    "    - UA池和代理池：\n",
    "    - selenium在scrapy中的应用：\n",
    "        - spider的init方法中声明一个浏览器对象的属性（浏览器对象必须要实例化一次）\n",
    "        - spider的closed（self，spider）方法中关闭浏览器对象\n",
    "        - 在中间件的process_response方法中获取浏览器对象（spider.bro）\n",
    "        - 在process_response方法中，获取页面源码数据（bro.page_source(包含了动态加载的数据)）\n",
    "        - 将源码数据赋值给HtmlResponse(url,body,request,encoding)的body参数\n",
    "    - crawlSpider：\n",
    "        - scrapy genspider -t crawl xxx www.xxx.com\n",
    "        - 连接提取器：LinkExtactor（allow='正则'）\n",
    "        - 规则解析器：Rule（link,callback,follow=True）\n",
    "- 分布式\n",
    "    - scrapy为何不能实现分布式：\n",
    "    - scrapy-redis的作用：\n",
    "    - 实现分布式的方式：\n",
    "    - 流程：\n",
    "- 增量式爬虫\n",
    "    - url:\n",
    "    - 数据：\n",
    "    \n",
    "- 提升scrapy爬取数据的效率：\n",
    "    - 增加并发\n",
    "    - 降低日志等级\n",
    "    - 禁止cookie\n",
    "    - 禁止重试\n",
    "    - 减少下载超时\n",
    "    \n",
    "    \n",
    "    \n",
    "    HttpCOnnectionPool:\n",
    "        - 代理ip\n",
    "        - Conection ： ‘close’\n",
    "        \n",
    "        xpath(xxx | xxx | xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 案例总结\n",
    "    - 爬取肯德基餐厅位置信息：http://www.kfc.com.cn/kfccda/index.aspx\n",
    "    - 爬取药监总局：http://125.35.6.84:81/xk/\n",
    "    - 爬取糗事百科图片：https://www.qiushibaike.com/pic/\n",
    "    - 下载免费简历模板：http://sc.chinaz.com/jianli/free.html\n",
    "    - 煎蛋网图片爬取：http://jandan.net/ooxx\n",
    "    - 解析城市名称：https://www.aqistudy.cn/historydata/\n",
    "    - 古诗文网：https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx\n",
    "    - 网易新闻：https://news.163.com/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反爬机制总结\n",
    "- robots.txt\n",
    "- UA检测\n",
    "- 验证码\n",
    "- 数据加密\n",
    "- cookie\n",
    "- 禁IP\n",
    "- 动态token\n",
    "- 数据动态加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清洗\n",
    "- 空值检测删除空值所在的行数据：\n",
    "    - isnull notnull any all\n",
    "    - 原理：\n",
    "        - 1. 将一组boolean值作为df的行索引（True保留）\n",
    "        - 2. 将空所在的行索引获取，使用drop（labels，axis=0）进行删除\n",
    "        - 3.dropna（axis=0）\n",
    "- 空值检测填充空值：\n",
    "    - df.fillna(method='ffill',axis=1)\n",
    "- 异常值检测和过滤：\n",
    "    - 判定异常值的条件   \n",
    "- 重复行检测和删除：\n",
    "    - drop_duplicated（keep=‘first/last’）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 面试题\n",
    "- 如何提升爬虫的效率\n",
    "    - 线程池\n",
    "    - scrapy配置文件相关配置（禁用cookie，禁止重试，减小下载超时，增加并发，日志等级）\n",
    "- scrapy核心组件工作流程\n",
    "- scrapy中如何设置代理（两种方法）\n",
    "    - 中间件\n",
    "    - 环境变量（os.environ['HTTPS_PROXY'] = 'https://ip:port'）\n",
    "- scrapy如何实现限速\n",
    "    - DOWNLOAD_DELAY = 1\n",
    "- scrapy如何实现暂停爬虫\n",
    "    - JOBDIR='sharejs.com'\n",
    "    - control-C\n",
    "- pipeline如何丢弃一个item对象\n",
    "- scrapy-redis组件作用\n",
    "- 实现深度和广度优先:默认为深度优先。\n",
    "    - DEPTH_PRIORITY = 1\n",
    "    - SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleFifoDiskQueue'\n",
    "    - SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.FifoMemoryQueue'\n",
    "    - 广度度优先：不全部保留结点，占用空间少；运行速度慢\n",
    "    - 深度优先：保留全部结点，占用空间大；运行速度快"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
